---
title: "class09"
author: "Xueyi Wan"
date: "2019/10/29"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Section 1 Exploratory data analysis

## perparing the data

data input
```{r}
wisc.df <- read.csv("WisconsinCancer.csv")
head(wisc.df)
```

Note that the 'id' and 'diagnosis' columns will not be used for most of the following steps

We have 'r norw(wisc.df)' samples in this dataset
```{r}
nrow(wisc.df)
```

How many benign (not cancerous) and maliganant (cancerous) samples do we have in the dataset
```{r}
table(wisc.df$diagnosis)
```

Use as.matrix() to convert the other features (i.e. columns) of the data (in columns 3 through 32) to a matrix.
```{r}
# Convert the features of the data: wisc.data
wisc.data <- as.matrix(wisc.df[, 3:32])
View(wisc.data)

# Set the row names of wisc.data
row.names(wisc.data) <- wisc.df$id
head(wisc.data)
```

Store te diagnosis for reference in the future as a separate vector
```{r}
# Create diagnosis vector by completing the missing code
diagnosis <- wisc.df$diagnosis
```

## Exploratory data analysis

Q1. How many observations are in this dataset?
```{r}
nrow(wisc.df)
```

Q2. How mnay variables/features in the data are suffixed with _mean?
```{r}
sum(grepl("_mean", colnames(wisc.df)))
```

Q3. How many of the observations have a mglignant diagnosis
```{r}
table(wisc.df$diagnosis)
```

# Section2 Principal Component Analysis

## Performing PCA

Check the mean and standard deviation of the features (i.e. columns) of the wisc.data to determine if the data should be scaled. 
```{r}
# Check column means and standard deviations
round(colMeans(wisc.data), 3)

round(apply(wisc.data,2,sd), 3)
```

The values look very different so we'll use `scale = TRUE`
```{r}
# Perform PCA on wisc.data
wisc.pr <- prcomp(wisc.data, scale = TRUE)

# Look at summary of results
summary(wisc.pr)
```

```{r}
plot(wisc.pr)
```

Lets make a plot of PC1 vs. PC2
```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2])
```

Colour by cancer/non-cancer...
```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col = diagnosis)
```

## Questions

Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?
```{r}
wisc.var <- wisc.pr$sdev^2
wisc.var.per <- round(wisc.var/sum(wisc.var)*100, 1)
wisc.var.per[1]
```

```{r}
x <- summary(wisc.pr)
x$importance[, "PC1"]
```


Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?
```{r}
sum(wisc.var.per[1:2])
sum(wisc.var.per[1:3])
```

```{r}
which(x$importance[3, ]>0.7)[1]
```


Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?
```{r}
sum(wisc.var.per[1:6])
sum(wisc.var.per[1:7])
```

```{r}
which(x$importance[3, ]>0.9)[1]
```


## Interpreting PCA results

Create a biplot of the wisc.pr using the biplot() function.
```{r}
biplot(wisc.pr)

# Scatter plot observations by components 1 and 2
plot(wisc.pr$x[, c(1, 2)], col = diagnosis, 
     xlab = "PC1", ylab = "PC2")

# Repeat for components 1 and 3
plot(wisc.pr$x[, c(1, 3)], col = (diagnosis + 1), 
     xlab = "PC1", ylab = "PC3")
```

## Variance explained

```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

```{r}
# Variance explained by each principal component: pve
pve <- pr.var/sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```


```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

## Communicating PCA results

Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?
```{r}
wisc.pr$rotation["concave.points_mean",1]
```

Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?
```{r}
which(x$importance[3,]>0.8)[1]
```

# Hierarchical clustering

## Hierarchical clustering of case data

```{r}
# Scale the wisc.data data: data.scaled
data.scaled <- scale(wisc.data)
```

```{r}
# Calculate the (Euclidean) distances between all pairs of observations in the new scaled dataset
data.dist <- dist(data.scaled)

# Create a hierarchical clustering model using complete linkage.
wisc.hclust <- hclust(data.dist, method = "complete")
```

## Result of hierarchical clustering

Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?
```{r}
plot(wisc.hclust)

# try and find the height for 4 clusters
table( cutree(wisc.hclust, h=19) )

# add the cut line
abline(h = 19, col="red", lty=2)
```


## Selecting number of clusters

Use cutree() to cut the tree so that it has 4 clusters. Assign the output to the variable wisc.hclust.clusters.
```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k = 4)
```

We can use the table() function to compare the cluster membership to the actual diagnoses.
```{r}
table(wisc.hclust.clusters, diagnosis)
```

Q12. Can you find a better cluster vs diagnoses match with by cutting into a different number of clusters between 2 and 10?

# K-means clustering

## K-means clustering and comparing results

Create a k-means model on wisc.data, assigning the result to wisc.km. Be sure to create 2 clusters, corresponding to the actual number of diagnosis. Also, remember to scale the data (with the scale() function and repeat the algorithm 20 times (by setting setting the value of the nstart argument appropriately).
```{r}
wisc.km <- kmeans(data.scaled, centers= 2, nstart= 20)
```

Use the table() function to compare the cluster membership of the k-means model (wisc.km$cluster) to the actual diagnoses contained in the diagnosis vector.
```{r}
table(wisc.km$cluster, diagnosis)
```

Use the table() function to compare the cluster membership of the k-means model (wisc.km$cluster) to your hierarchical clustering model from above (wisc.hclust.clusters). 
```{r}
table(wisc.hclust.clusters, wisc.km$cluster)
```

# Combining methods

## Clustering on PCA results

Using the minimum number of principal components required to describe at least 90% of the variability in the data, create a hierarchical clustering model with the linkage method="ward.D2".

We use Ward’s criterion here because it is based on multidimensional variance like principal components analysis. Assign the results to wisc.pr.hclust.
```{r}
# Use the distance along the first 7 PCs for clustering 
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method = "ward.D2")
plot(wisc.pr.hclust)
```

```{r}
#Cut this hierarchical clustering model into 2 clusters and assign the results to wisc.pr.hclust.clusters.
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

Q14. How well does the newly created model with four clusters separate out the two diagnoses?
```{r}
# Compare to actual diagnoses
table(grps, diagnosis)
```

```{r}
plot(wisc.pr$x[, 1:2], col = grps)
plot(wisc.pr$x[,1:2], col=diagnosis)

#Note the color swap here as the hclust cluster 1 is mostly “M” and cluster 2 is mostly “B” as we saw from the results of calling table(grps, diagnosis). 
```

To match things up we can turn our groups into a factor and reorder the levels so cluster 2 comes first
```{r}
g <- as.factor(grps)
levels(g)
```

```{r}
g <- relevel(g,2)
levels(g)
```

```{r}
# Plot using our re-ordered factor 
plot(wisc.pr$x[,1:2], col=g)
```

Q14. How well does the newly created model with four clusters separate out the two diagnoses?
```{r}
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)

# Compare to actual diagnoses
wisc.pr.hclust.clusters.compare <-  table(wisc.pr.hclust.clusters, diagnosis)
wisc.pr.hclust.clusters.compare
```

Q15. How well do the k-means and hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model
```{r}
wisc.km.cluster.compare <- table(wisc.km$cluster, diagnosis)
wisc.km.cluster.compare

wisc.hclust.clusters.compare <- table(wisc.hclust.clusters, diagnosis)
wisc.hclust.clusters.compare
```


# Section 6 Sensitivity/Specificity

Q16. Which of your analysis procedures resulted in a clustering model with the best specificity? How about sensitivity?
```{r}
# sensitivity: correctly detected M / total diagnosied M
sensitivity  <- function(table, cluster){
  sens.data <- table[cluster, "M"]/sum(table[, "M"])
  return(sens.data)
}

sensitivity(wisc.pr.hclust.clusters.compare, 1)
sensitivity(wisc.km.cluster.compare, 2)
sensitivity(wisc.hclust.clusters.compare, 1)
```

```{r}
# specificity: correctly detected B / total diagnosied B
specificity  <- function(table, cluster){
  spec.data <- table[cluster, "B"]/sum(table[, "B"])
  return(spec.data)
}

specificity(wisc.pr.hclust.clusters.compare, 2)
specificity(wisc.km.cluster.compare, 1)
specificity(wisc.hclust.clusters.compare, 3)
```


# Section 7 Prediction

We will use the predict() function that will take our PCA model from before and new cancer cell data and project that data onto our PCA space.
```{r}
url <- "new_samples.csv"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```

```{r}
plot(wisc.pr$x[,1:2], col=g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

# About this document
```{r}
sessionInfo()
```

